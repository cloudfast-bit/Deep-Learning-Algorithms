{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Random variables and Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue May  1 15:12:54 2018\n",
    "\n",
    "@author: basharm\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "#SEED = 100\n",
    "SEED = 123\n",
    "\n",
    "#reference: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(SEED)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "re1 = re.compile(r' +')\n",
    "\n",
    "def textFixup(aText):\n",
    "    aText = aText.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ').replace('â€™', \"'\")\n",
    "    return re1.sub(' ', html.unescape(aText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#r_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_aTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = textFixup(tweet)\n",
    "    #tokens = r_tokenizer.tokenize(tweet)\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [p_stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data_and_labels_csv(fileLoc):\n",
    "    examples = []\n",
    "    labels = []\n",
    "    df = pd.read_csv(fileLoc)\n",
    "    for i in df.index:\n",
    "        examples.append(preprocess_aTweet(df['text'][i]))\n",
    "        if df['label'][i] == 0:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return examples, labels\n",
    "    \n",
    "X_train, y_train = load_data_and_labels_csv('U:\\\\Research\\\\Projects\\\\sef\\\\datamining\\\\mlonlineabuse\\\\WorkingFolder\\\\Train\\\\train_final.csv')\n",
    "\n",
    "X_test, y_test = load_data_and_labels_csv('U:\\\\Research\\\\Projects\\\\sef\\\\datamining\\\\mlonlineabuse\\\\WorkingFolder\\\\Test\\\\test_final.csv')\n",
    "\n",
    "ytrain = np.array(y_train)\n",
    "ytest = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "count_vectorizer.fit(X_train)\n",
    "#count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.transform(X_train)\n",
    "X_train_vectors = train_vectors.toarray()\n",
    "test_vectors = count_vectorizer.transform(X_test)\n",
    "X_test_vectors = test_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = list([0,1])\n",
    "y_train_vectors = []\n",
    "for e in ytrain:\n",
    "    output_empty = [0] * len(categories)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(e)] = 1\n",
    "    y_train_vectors.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_vectors = []\n",
    "for e in ytest:\n",
    "    output_empty = [0] * len(categories)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(e)] = 1\n",
    "    y_test_vectors.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DNN model and training it for 10 epoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4374  | total loss: \u001b[1m\u001b[32m0.05876\u001b[0m\u001b[0m | time: 0.412s\n",
      "| Adam | epoch: 035 | loss: 0.05876 - acc: 0.9773 -- iter: 3968/3988\n",
      "Training Step: 4375  | total loss: \u001b[1m\u001b[32m0.05380\u001b[0m\u001b[0m | time: 0.415s\n",
      "| Adam | epoch: 035 | loss: 0.05380 - acc: 0.9796 -- iter: 3988/3988\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "def create_dnn_model():\n",
    "    \n",
    "    inputSize = len(X_train_vectors[0])\n",
    "    outputSize = len(y_train_vectors[0])\n",
    "    \n",
    "    # reset underlying graph data\n",
    "    tf.reset_default_graph()\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, inputSize])\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = Dropout(0.5)(net)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, outputSize, activation='softmax')\n",
    "    net = tflearn.regression(net,learning_rate=0.04)\n",
    "\n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "    \n",
    "    return model\n",
    "\n",
    "dnn_model = create_dnn_model()\n",
    "dnn_model.fit(X_train_vectors, y_train_vectors, n_epoch=35, batch_size=32, show_metric=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive\t146\n",
      "True Negative\t664\n",
      "False Positive\t97\n",
      "False Negative\t100\n",
      "Accuracy\t0.8043694141012909\n",
      "Precision\t0.6008230452674898\n",
      "Recall\t0.5934959349593496\n",
      "f-measure\t0.5971370143149285\n",
      "cohen_kappa_score\t0.46796310748872927\n",
      "auc\t0.7330160358108181\n",
      "roc_auc\t0.7330160358108181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "actual = np.argmax(y_test_vectors, axis=1)\n",
    "#print(actual)\n",
    "predictions = dnn_model.predict(X_test_vectors)\n",
    "predicted = np.argmax(predictions, axis=1)\n",
    "predicted = np.array(predicted)\n",
    "\n",
    "tp = np.count_nonzero(predicted * actual)\n",
    "tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp = np.count_nonzero(predicted * (actual - 1))\n",
    "fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "print('True Positive\\t' + str(tp))\n",
    "print('True Negative\\t' + str(tn))\n",
    "print('False Positive\\t' + str(fp))\n",
    "print('False Negative\\t' + str(fn))\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "cohen_kappa_score = cohen_kappa_score(predicted, actual)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(actual, predicted)\n",
    "auc_val = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_val = roc_auc_score(actual, predicted)\n",
    "\n",
    "print('Accuracy\\t' + str(accuracy))\n",
    "print('Precision\\t' + str(precision))\n",
    "print('Recall\\t' + str(recall))\n",
    "print('f-measure\\t' + str(fmeasure))\n",
    "print('cohen_kappa_score\\t' + str(cohen_kappa_score))\n",
    "print('auc\\t' + str(auc_val))\n",
    "print('roc_auc\\t' + str(roc_auc_val))\n",
    "\n",
    "#print(\"Average of ROC-AUC score: %.3f\" % roc_auc_score(ytest, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

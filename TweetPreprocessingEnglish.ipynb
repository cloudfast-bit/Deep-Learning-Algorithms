{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Preprocessing Using ekphrasis\n",
    "ekphrasis Link: https://github.com/cbaziotis/ekphrasis\n",
    "\n",
    "ekphrasis offers the following functionality:\n",
    "\n",
    "1. Social Tokenizer. A text tokenizer geared towards social networks (Facebook, Twitter...), which understands complex emoticons, emojis and other unstructured expressions like dates, times and more.\n",
    "\n",
    "2. Word Segmentation. You can split a long string to its constituent words. Suitable for hashtag segmentation.\n",
    "\n",
    "3. Spell Correction. You can replace a misspelled word, with the most probable candidate word.\n",
    "\n",
    "4. Customization. Taylor the word-segmentation, spell-correction and term identification, to suit your needs.\n",
    "\n",
    "Word Segmentation and Spell Correction mechanisms, operate on top of word statistics, collected from a given corpus. We provide word statistics from 2 big corpora (from Wikipedia and Twitter), but you can also generate word statistics from your own corpus. You may need to do that if you are working with domain-specific texts, like biomedical documents. For example a word describing a technique or a chemical compound may be treated as a misspelled word, using the word statistics from a general purposed corpus.\n",
    "\n",
    "ekphrasis tokenizes the text based on a list of regular expressions. You can easily enable ekphrasis to identify new entities, by simply adding a new entry to the dictionary of regular expressions (ekphrasis/regexes/expressions.txt).\n",
    "\n",
    "5. Pre-Processing Pipeline. You can combine all the above steps in an easy way, in order to prepare the text files in your dataset for some kind of analysis or for machine learning. In addition, to the aforementioned actions, you can perform text normalization, word annotation (labeling) and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ekphrasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Text Pre-Processing pipeline\n",
    "\n",
    "You can easily define a preprocessing pipeline, by using the TextPreProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Some tweets\n",
    "\n",
    "Notes:\n",
    "1. elongated words are automatically normalized.\n",
    "2. Spell correction affects performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
    "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
    "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<allcaps> cant wait </allcaps> for the new season of <hashtag> twin peaks </hashtag> ＼(^o^)／ ! <repeated> <hashtag> david lynch </hashtag> <hashtag> tv series </hashtag> <happy>\n",
      "\n",
      "i saw the new <hashtag> john doe </hashtag> movie and it sucks <elongated> ! <repeated> <allcaps> waisted </allcaps> <money> . <repeated> <hashtag> bad movies </hashtag> <annoyed>\n",
      "\n",
      "<user> : can not wait for the <date> <hashtag> sentiment </hashtag> talks ! <allcaps> yay <elongated> </allcaps> ! <repeated> <laugh> <url>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print(\" \".join(text_processor.pre_process_doc(s)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fins = ['EI-oc-En-train\\\\EI-oc-En-fear-train.txt', '2018-EI-oc-En-dev\\\\2018-EI-oc-En-fear-dev.txt', '2018-EI-oc-En-test\\\\2018-EI-oc-En-fear-test.txt']\n",
    "# fouts = ['EI-oc-En-train\\\\PrePro_EI-oc-En-fear-train.txt', '2018-EI-oc-En-dev\\\\PrePro_2018-EI-oc-En-fear-dev.txt', '2018-EI-oc-En-test\\\\PrePro_2018-EI-oc-En-fear-test.txt']\n",
    "\n",
    "#BASE = 'D:\\\\ResearchDataGtx1060\\\\SentimentData\\\\Harasment\\\\Sharing Data\\\\'\n",
    "# fins = ['Racial Data.csv', 'Sextual Data.csv', 'Political Data.csv', 'Intelligence Data.csv', 'Appearance Data.csv']\n",
    "# fouts = ['PrePro_Racial.csv', 'PrePro_Sextua.csv', 'PrePro_Political.csv', 'PrePro_Intelligence.csv', 'PrePro_Appearance.csv']\n",
    "\n",
    "# BASE = 'D:\\\\ResearchDataGtx1060\\\\SentimentData\\\\Racism\\\\'\n",
    "# fins = ['NAACL_SRW_2016_Tweets.csv']\n",
    "# fouts = ['PrePro_NAACL_SRW_2016_Tweets.csv']\n",
    "\n",
    "# BASE = 'D:\\\\ResearchDataGtx1060\\\\MisInformation\\\\Thiru\\\\'\n",
    "# fins = ['final_tweets_share.csv', 'poynter.csv']\n",
    "# fouts = ['PrePro_final_tweets_share.csv', 'PrePro_poynter.csv']\n",
    "\n",
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\SentimentData\\\\Hate\\\\random-hate\\\\'\n",
    "fins = ['train_E6oV3lV.csv', 'test_tweets_anuFYb8.csv']\n",
    "fouts = ['PrePro_train_E6oV3lV.csv', 'PrePro_test_tweets_anuFYb8.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31968</td>\n",
       "      <td>choose to be   :) #momtips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31969</td>\n",
       "      <td>something inside me dies Ã°ÂÂÂ¦Ã°ÂÂÂ¿Ã¢ÂÂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31970</td>\n",
       "      <td>#finished#tattoo#inked#ink#loveitÃ¢ÂÂ¤Ã¯Â¸Â ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31971</td>\n",
       "      <td>@user @user @user i will never understand why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31972</td>\n",
       "      <td>#delicious   #food #lovelife #capetown mannaep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31973</td>\n",
       "      <td>1000dayswasted - narcosis infinite ep.. make m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31974</td>\n",
       "      <td>one of the world's greatest spoing events   #l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31975</td>\n",
       "      <td>half way through the website now and #allgoing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31976</td>\n",
       "      <td>good food, good life , #enjoy and   Ã°ÂÂÂÃ°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31977</td>\n",
       "      <td>i'll stand behind this #guncontrolplease   #se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31978</td>\n",
       "      <td>i ate,i ate and i ate...Ã°ÂÂÂÃ°ÂÂÂ   #ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31979</td>\n",
       "      <td>@user got my @user limited edition rain or sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31980</td>\n",
       "      <td>&amp;amp; #love &amp;amp; #hugs &amp;amp; #kisses too! how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31981</td>\n",
       "      <td>Ã°ÂÂÂ­Ã°ÂÂÂÃ°ÂÂÂ #girls   #sun #fave @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31982</td>\n",
       "      <td>thought factory: bbc neutrality on right wing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              tweet\n",
       "0   31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1   31964   @user #white #supremacists want everyone to s...\n",
       "2   31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3   31966  is the hp and the cursed child book up for res...\n",
       "4   31967    3rd #bihday to my amazing, hilarious #nephew...\n",
       "5   31968                        choose to be   :) #momtips \n",
       "6   31969  something inside me dies Ã°ÂÂÂ¦Ã°ÂÂÂ¿Ã¢ÂÂ...\n",
       "7   31970  #finished#tattoo#inked#ink#loveitÃ¢ÂÂ¤Ã¯Â¸Â ...\n",
       "8   31971   @user @user @user i will never understand why...\n",
       "9   31972  #delicious   #food #lovelife #capetown mannaep...\n",
       "10  31973  1000dayswasted - narcosis infinite ep.. make m...\n",
       "11  31974  one of the world's greatest spoing events   #l...\n",
       "12  31975  half way through the website now and #allgoing...\n",
       "13  31976  good food, good life , #enjoy and   Ã°ÂÂÂÃ°...\n",
       "14  31977  i'll stand behind this #guncontrolplease   #se...\n",
       "15  31978  i ate,i ate and i ate...Ã°ÂÂÂÃ°ÂÂÂ   #ja...\n",
       "16  31979   @user got my @user limited edition rain or sh...\n",
       "17  31980  &amp; #love &amp; #hugs &amp; #kisses too! how...\n",
       "18  31981  Ã°ÂÂÂ­Ã°ÂÂÂÃ°ÂÂÂ #girls   #sun #fave @...\n",
       "19  31982  thought factory: bbc neutrality on right wing ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track=1\n",
    "df_one = pd.read_csv(BASE+fins[track], sep=',', encoding='latin1')\n",
    "print(len(df_one))\n",
    "df_one.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_one['misinfo'] = df_one['misinfo'].str.lower().str.strip()\n",
    "#df_one.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_one['facts'] = df_one['facts'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 1\n",
    "for idx in df_one.index:\n",
    "    sent = df_one.loc[idx,'tweet']\n",
    "    sent = sent.replace('‘', '\\'').replace('’', '\\'').replace('“', '\"').replace('”', '\"')\n",
    "    #print(sent)\n",
    "    sent = ' '.join(text_processor.pre_process_doc(sent))\n",
    "    #print(sent)\n",
    "    df_one.loc[idx,'tweet'] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>&lt;hashtag&gt; studio life &lt;/hashtag&gt; &lt;hashtag&gt; a i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>&lt;user&gt; &lt;hashtag&gt; white &lt;/hashtag&gt; &lt;hashtag&gt; su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your &lt;hashtag&gt; acne &lt;/hashta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3 rd &lt;hashtag&gt; bih day &lt;/hashtag&gt; to my amazin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31968</td>\n",
       "      <td>choose to be &lt;happy&gt; &lt;hashtag&gt; mom tips &lt;/hash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31969</td>\n",
       "      <td>something inside me dies ã ° â  â  â ¦ ã ° â...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31970</td>\n",
       "      <td>&lt;hashtag&gt; finished &lt;/hashtag&gt; &lt;hashtag&gt; tattoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31971</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i will never understand w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31972</td>\n",
       "      <td>&lt;hashtag&gt; delicious &lt;/hashtag&gt; &lt;hashtag&gt; food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31973</td>\n",
       "      <td>1 0 0 0 dayswasted - narcosis infinite ep . &lt;r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31974</td>\n",
       "      <td>one of the world ' s greatest spoing events &lt;h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31975</td>\n",
       "      <td>half way through the website now and &lt;hashtag&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31976</td>\n",
       "      <td>good food , good life , &lt;hashtag&gt; enjoy &lt;/hash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31977</td>\n",
       "      <td>i will stand behind this &lt;hashtag&gt; gun control...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>31978</td>\n",
       "      <td>i ate , i ate and i ate . &lt;repeated&gt; ã ° â  â...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31979</td>\n",
       "      <td>&lt;user&gt; got my &lt;user&gt; limited edition rain or s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31980</td>\n",
       "      <td>&amp; &lt;hashtag&gt; love &lt;/hashtag&gt; &amp; &lt;hashtag&gt; hugs &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31981</td>\n",
       "      <td>ã ° â  â  â ­ ã ° â  â  â  ã ° â  â  â ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31982</td>\n",
       "      <td>thought factory : bbc neutrality on right wing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              tweet\n",
       "0   31963  <hashtag> studio life </hashtag> <hashtag> a i...\n",
       "1   31964  <user> <hashtag> white </hashtag> <hashtag> su...\n",
       "2   31965  safe ways to heal your <hashtag> acne </hashta...\n",
       "3   31966  is the hp and the cursed child book up for res...\n",
       "4   31967  3 rd <hashtag> bih day </hashtag> to my amazin...\n",
       "5   31968  choose to be <happy> <hashtag> mom tips </hash...\n",
       "6   31969  something inside me dies ã ° â  â  â ¦ ã ° â...\n",
       "7   31970  <hashtag> finished </hashtag> <hashtag> tattoo...\n",
       "8   31971  <user> <user> <user> i will never understand w...\n",
       "9   31972  <hashtag> delicious </hashtag> <hashtag> food ...\n",
       "10  31973  1 0 0 0 dayswasted - narcosis infinite ep . <r...\n",
       "11  31974  one of the world ' s greatest spoing events <h...\n",
       "12  31975  half way through the website now and <hashtag>...\n",
       "13  31976  good food , good life , <hashtag> enjoy </hash...\n",
       "14  31977  i will stand behind this <hashtag> gun control...\n",
       "15  31978  i ate , i ate and i ate . <repeated> ã ° â  â...\n",
       "16  31979  <user> got my <user> limited edition rain or s...\n",
       "17  31980  & <hashtag> love </hashtag> & <hashtag> hugs <...\n",
       "18  31981  ã ° â  â  â ­ ã ° â  â  â  ã ° â  â  â ...\n",
       "19  31982  thought factory : bbc neutrality on right wing..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_one = df_one[['facts', 'misinfo']]\n",
    "# df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.to_csv(BASE+fouts[track], index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

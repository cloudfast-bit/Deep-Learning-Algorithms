{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Explained Step by Step\n",
    "\n",
    "In this notebook, from very high level, we show what is happening in a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "\n",
    "# neural network libraries\n",
    "import tensorflow as tf # tensorflow backend\n",
    "from keras.layers import Input # for input layer\n",
    "from keras.layers.embeddings import Embedding # for embedding\n",
    "from keras.layers import import Dropout # for random dropout\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D # for convolution layer\n",
    "from keras.layers import concatenate # for concatenation\n",
    "from keras.layers import Activation # for activation layer\n",
    "from keras.layers import Dense # for fully connected layer\n",
    "from keras.models import Model # Model groups layers into an object with training and inference features.\n",
    "\n",
    "# libraries for data formatting\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# word embedding loading library\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# custom libraries\n",
    "from model_persistance import ModelPersistance\n",
    "from evaluate_classification import EvaluateBinaryClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Random variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input sources\n",
    "BASE = 'D:\\\\ResearchDataGtx1060\\\\SentimentData\\\\Racism\\\\'\n",
    "fins_train = ['train.csv']\n",
    "fins_test = ['test.csv']\n",
    "track = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt &lt;user&gt; : deconstructed lemon tart . basical...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>argh &lt;elongated&gt; i want to kick in the televis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;hashtag&gt; mkr &lt;/hashtag&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. &lt;user&gt; no , no . &lt;repeated&gt; this is my view ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am just so embarrassed for her . &lt;hashtag&gt; m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  rt <user> : deconstructed lemon tart . basical...      0\n",
       "1  argh <elongated> i want to kick in the televis...      1\n",
       "2                           <hashtag> mkr </hashtag>      0\n",
       "3  . <user> no , no . <repeated> this is my view ...      1\n",
       "4  i am just so embarrassed for her . <hashtag> m...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train data in a dataframe\n",
    "df_train = pd.read_csv(BASE+fins_train[track])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "label      \n",
       "0      2763\n",
       "1       857"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check howmany positive and negative examples are there\n",
    "df_train.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3620"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the total number of examples\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate independent variabe (text) and dependednt variable (label)\n",
    "X_train, y_train = df_train['text'].values, df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rt <user> : deconstructed lemon tart . basically a pile of crap on a plate <hashtag> mkr </hashtag> <url>',\n",
       "       'argh <elongated> i want to kick in the television set right now , kat you despicable rat <hashtag> mkr </hashtag>',\n",
       "       '<hashtag> mkr </hashtag>',\n",
       "       '. <user> no , no . <repeated> this is my view on how to move equality forward . must root out those who claim equality while working against it .',\n",
       "       'i am just so embarrassed for her . <hashtag> mkr </hashtag>'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what we got in independent variable (text)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what we got in dependent variabl\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;hashtag&gt; mkr &lt;/hashtag&gt; yum there cooking up ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;url&gt; / / &lt;user&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; why is kat being so nasty ? just showin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; if katie and nikki scored a point for e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; i ' d shopped off the website before &lt;h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  <hashtag> mkr </hashtag> yum there cooking up ...      0\n",
       "1                                   <url> / / <user>      0\n",
       "2  <user> why is kat being so nasty ? just showin...      1\n",
       "3  <user> if katie and nikki scored a point for e...      0\n",
       "4  <user> i ' d shopped off the website before <h...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read test data\n",
    "df_test = pd.read_csv(BASE+fins_test[track])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate variabes of test data\n",
    "X_test, y_test = df_test['text'].values, df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data suitable for model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 2, 160, 148, 176, 705, 6, 1041, 15, 417, 24, 6, 260, 1, 3, 1, 23], [1501, 89, 4, 97, 8, 896, 21, 5, 1042, 442, 123, 90, 34, 12, 3014, 3015, 1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "num_words = 100000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "xtrain = tokenizer.texts_to_sequences(X_train) # coverts text to numbers\n",
    "print(xtrain[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0   14    2  160  148\n",
      "   176  705    6 1041   15  417   24    6  260    1    3    1   23]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0 1501   89    4   97    8  896\n",
      "    21    5 1042  442  123   90   34   12 3014 3015    1    3    1]]\n"
     ]
    }
   ],
   "source": [
    "maxlen = max(map(lambda x: len(x), xtrain))\n",
    "xtrain = pad_sequences(xtrain, maxlen=maxlen) # padding each row to make them same length\n",
    "print(xtrain[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text of test data\n",
    "xtest = tokenizer.texts_to_sequences(X_test) # coverts text to numbers\n",
    "xtest = pad_sequences(xtest, maxlen=maxlen) # padding each row to make them same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading word embedding and mapping data to that word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_BASE = 'D:\\\\ResearchDataGtx1060\\\\TwitterDataAustralia\\\\\\W2V_AusTweets_200d_MinCount100\\\\'\n",
    "model_ug_cbow = KeyedVectors.load(W2V_BASE+'vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad',\n",
       " 'australia',\n",
       " 'anti',\n",
       " 'science',\n",
       " 'experts',\n",
       " 'government',\n",
       " 'america',\n",
       " 'corona',\n",
       " 'virus',\n",
       " 'breaking']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 20 words in the word embedding list\n",
    "list(model_ug_cbow.wv.vocab.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need an embedding matrix to use in our cnn. \n",
    "# first, let's  have a list of word to embedding vector map\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_ug_cbow.wv[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0316982e+00,  2.1833405e-01, -5.6165594e-01,  1.9352080e-01,\n",
       "       -9.3115920e-01, -5.2442926e-01, -4.1290745e-01, -1.1854973e-01,\n",
       "        8.6853719e-01, -8.1930059e-01, -6.0529087e-02,  5.9996063e-01,\n",
       "        3.3220369e-02, -3.7161040e-01,  8.7345284e-01,  6.0169494e-01,\n",
       "       -1.9462686e+00,  5.9856260e-01,  6.3711649e-01,  1.7781970e-01,\n",
       "       -3.1967349e-03, -3.7669054e-01, -6.0284066e-01,  1.4469020e-01,\n",
       "       -4.8053089e-01, -1.7896198e+00, -9.6595472e-01,  9.1273896e-02,\n",
       "       -8.8656074e-01, -5.3717041e-01,  4.8754922e-01,  8.8729465e-01,\n",
       "        8.0223280e-01, -2.0967886e+00,  7.5208932e-01,  3.0945593e-01,\n",
       "       -1.5034870e+00,  3.0100110e-01, -1.5653280e+00, -6.6279548e-01,\n",
       "        8.9251941e-01,  1.3549447e-01, -3.9278197e-01,  1.3906772e-03,\n",
       "        5.0559813e-01,  8.8537502e-01,  8.4796727e-01, -1.2684748e+00,\n",
       "        9.9836671e-01,  1.4374197e+00,  1.2900113e+00,  1.2452897e+00,\n",
       "       -7.0647120e-01,  2.0335373e-01,  1.1188688e+00,  2.6864177e-01,\n",
       "        3.5028729e-01, -2.8955153e-01, -6.6908643e-02,  1.6199923e-01,\n",
       "        7.0815915e-01,  1.0030941e+00,  6.3357574e-01, -4.3168265e-01,\n",
       "       -3.8484207e-01,  7.1587622e-02, -7.8871864e-01, -3.7518057e-01,\n",
       "       -6.5351516e-01, -8.8305718e-01,  1.4000481e+00, -4.8676884e-01,\n",
       "       -1.1858699e+00,  8.1754196e-01,  8.8761961e-01,  1.1562160e+00,\n",
       "        4.7459161e-01, -1.7157663e+00,  1.3537985e+00,  1.0633346e+00,\n",
       "        6.9346651e-02,  4.3589237e-01,  1.3198330e-01, -1.6439357e+00,\n",
       "        2.9944816e-01,  8.0196492e-02,  6.7064703e-01, -3.6355644e-01,\n",
       "        3.5202098e-01, -3.4820154e-01, -1.9617970e+00, -4.8782781e-01,\n",
       "       -7.5025886e-01,  8.3843060e-02,  1.6817182e+00, -4.6326786e-01,\n",
       "        7.5676662e-01,  3.8432088e-01,  1.0823857e+00,  4.1177937e-01,\n",
       "       -1.4640998e+00, -1.1698884e+00,  4.9900490e-01,  6.0429883e-01,\n",
       "        5.1866621e-01,  2.1716948e-01, -3.6530986e-02,  5.1414132e-01,\n",
       "       -3.7213981e-01,  2.8172526e-01,  1.8820816e-01, -1.7086399e+00,\n",
       "       -1.3336980e-01, -9.3795168e-01,  2.7625147e-01,  6.8530250e-01,\n",
       "        5.4879761e-01, -4.7950232e-01, -1.0425373e+00, -1.0468755e+00,\n",
       "        8.4448224e-01, -5.8134693e-01,  3.4018192e-01,  1.1539515e+00,\n",
       "       -2.8065091e-01,  1.3108538e-01,  1.4754748e+00,  1.1824049e-01,\n",
       "       -2.8970987e-01,  2.1800403e-01, -5.2525902e-01, -1.6546474e-01,\n",
       "       -7.4933243e-01, -3.4221584e-01, -6.6864163e-01,  1.4146464e+00,\n",
       "       -1.0438733e+00,  1.1821837e+00,  5.8647358e-01,  5.9797138e-01,\n",
       "        5.2412689e-01, -1.4070324e+00, -4.8810372e-01,  6.7131662e-01,\n",
       "       -3.4696108e-01, -3.7018087e-02, -8.6773044e-01, -8.0097800e-01,\n",
       "        1.2644584e-01, -2.2589321e-01, -7.8058702e-01, -4.6647381e-02,\n",
       "        1.1517155e+00, -2.3060999e+00,  3.7892604e-01, -9.0173587e-02,\n",
       "        5.3421432e-01, -4.7215191e-01,  6.3761950e-02, -5.6682783e-01,\n",
       "       -4.6423957e-01, -1.0487992e+00, -8.5109770e-01,  1.2830450e-01,\n",
       "       -7.0096171e-01,  7.9901487e-01,  8.0431777e-01, -1.0038173e+00,\n",
       "       -1.5452116e+00,  4.3398860e-01,  1.1027994e+00, -1.9723709e+00,\n",
       "       -1.4763364e-01, -9.1398859e-01, -1.3576195e+00,  2.4908146e-01,\n",
       "        2.1596362e-01,  5.3123909e-01, -2.9992032e-01,  5.3466469e-01,\n",
       "       -2.6893654e-01,  3.3467332e-01,  2.4750808e-02, -6.1385912e-01,\n",
       "        4.0176836e-01,  6.8243957e-01,  3.2408640e-01,  2.3501787e-01,\n",
       "       -1.0596597e+00,  1.2332005e-01,  4.6967888e-01,  5.8338571e-01,\n",
       "        3.1983742e-01, -1.5802751e-01,  1.9538875e-01, -3.8021782e-01,\n",
       "        1.3162018e-01, -1.5461199e+00,  1.6640526e+00, -2.6158670e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the embedding for the work 'bad'\n",
    "embeddings_index['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second, creat an empty embedding matrix with all zeros\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "embedding_matrix[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.73670197,  0.45571646, -0.27791244,  0.88261896,  1.04669583,\n",
       "         0.16770193,  0.43549842, -0.27640629, -0.25978848,  1.44726074,\n",
       "         0.20872121, -0.09048253, -1.07952297,  0.02231651,  0.10331074,\n",
       "        -1.67800665, -0.82747835, -0.93844092,  0.4275189 ,  1.71133602,\n",
       "        -0.69659722, -2.39545155, -0.78943467,  0.06259736, -1.05729604,\n",
       "         0.19098742, -2.1114645 ,  1.18377471, -2.1522553 ,  1.13574374,\n",
       "         0.54996145, -1.54511523, -0.99421501, -0.49631831,  1.41372657,\n",
       "         0.48158616, -0.97559386, -0.15809409, -0.05576934, -1.14133978,\n",
       "         1.17335463,  0.40654907,  0.05706169,  0.32113919,  0.84400076,\n",
       "        -1.31370902, -0.5564608 , -0.32810634, -0.28290322,  0.5432182 ,\n",
       "        -0.40610105, -0.7165457 ,  0.59209287,  0.84504771,  0.01761631,\n",
       "         0.43542054, -0.58775425,  0.00441035,  1.1079545 , -0.45609498,\n",
       "         0.04078272, -0.22422069,  1.04007506, -0.57973063, -0.68684018,\n",
       "         1.52067685,  0.08966362,  0.45031899, -1.98507154,  1.93917549,\n",
       "        -0.20130065,  1.17864215, -0.0158907 , -2.17216325,  0.06194538,\n",
       "         0.14930187, -1.03085065, -0.57575923,  1.31511307, -0.28845048,\n",
       "         0.0235661 , -2.17607713, -0.6134249 ,  0.4545359 ,  0.87082219,\n",
       "        -0.92523056, -0.27022517,  0.08293665,  0.95616794,  0.42378861,\n",
       "        -0.23152384, -0.18687972,  0.3422085 ,  0.31060266,  0.80243564,\n",
       "        -0.51619995, -0.48936686, -0.5099656 ,  1.19514763, -1.96995282,\n",
       "        -0.35859078,  0.14203414, -1.71530437, -0.1314414 , -1.61370754,\n",
       "        -0.83555102,  0.7304529 ,  0.10200679,  1.5305624 , -0.85053158,\n",
       "         0.96754688, -0.57166487, -1.20036054, -1.73841858, -0.11500877,\n",
       "         0.31204993,  0.53970194,  0.08175413,  0.29708984, -0.68596143,\n",
       "         0.22757234, -0.44426188, -1.1432755 , -0.06688846,  0.12114069,\n",
       "         1.23759735,  0.69212842, -2.77661467, -0.53402293, -0.42587429,\n",
       "        -1.68976235,  1.24371541, -1.54577267, -0.45195794,  0.7298516 ,\n",
       "        -1.7990582 , -0.86734664, -2.07891417,  0.39361793,  0.68608665,\n",
       "         0.66526407,  0.62619555, -1.43749499, -1.59003603, -2.66749978,\n",
       "        -0.19244854,  0.41369829,  1.54353344, -0.43506306, -0.46469903,\n",
       "         0.32759193, -0.12557425, -0.42013833, -0.01015022, -0.47149137,\n",
       "        -0.23342094,  0.07495465,  0.84303731, -1.54571474, -0.36639312,\n",
       "        -0.67286849, -1.00465333,  0.66002023, -0.76271033, -0.81645912,\n",
       "         0.00565197,  0.05503653, -0.17993005,  1.37079597, -1.24170327,\n",
       "         1.56372869,  0.93902725, -1.75513923,  1.17989862,  1.06133771,\n",
       "         0.77145743, -0.73036873,  0.07784334, -0.44872028,  1.14239085,\n",
       "        -0.31785828,  0.86060435, -0.82659847, -0.69942993,  0.38632667,\n",
       "        -0.38057077,  0.56260568, -0.54855174,  0.03420892, -0.8109507 ,\n",
       "         1.79283738, -1.55454433,  0.34764823,  0.9674207 , -1.27961946,\n",
       "        -1.67535937,  1.13049865,  0.85917753,  1.4211179 ,  0.04729427]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third, for each workd in our training dataset vocabulary (i.e. in tokenizer),\n",
    "# put the corresponding word embeeding to the matrix\n",
    "# if a word is not in the pretrained embedding, the corresponding entry will remain zeros\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CNN model and training it for n epocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(None, 69) dtype=int32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "tweet_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "someone coming from C/C++/Java programming background may find inner fucntion of python a bit confusing. \n",
    "Therefore, we need to explain it before going to the next line of code.\n",
    "\n",
    "f(a, b) and f(a)(b) are not the same thing in pthon:\n",
    "f(a, b) is a simple function that takes two parameters, i.e. f(a, b) calls f with two parameters a and b.\n",
    "f(a)(b) is a nested function that takes one parameter for outer fucnation and one parametr for inner function. \n",
    "That is, f(a)(b) calls f with one parameter a, which then returns another function, \n",
    "which is then called with one parameter b. Consider the following nested fucntion for example:\n",
    "\n",
    "def func(a):\n",
    "    def func2(b):\n",
    "        return a + b\n",
    "    return func2\n",
    "    \n",
    "When you call 'func()' it returns the inner functon 'func2'. Then you call that inncer function.\n",
    ">>func2 = func(1)\n",
    ">>func2(2)\n",
    ">>3\n",
    "\n",
    "If you don't need the inner function later on, then there's no need to save it into a variable. \n",
    "You can just call them one after the other.\n",
    ">>func(1)(2)\n",
    ">>3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_1/embedding_lookup/Identity_1:0' shape=(None, 69, 200) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_encoder = Embedding(num_words, \n",
    "                          200, \n",
    "                          weights=[embedding_matrix], \n",
    "                          input_length=maxlen, \n",
    "                          trainable=True)(tweet_input)\n",
    "tweet_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_1/cond/Identity:0' shape=(None, 69, 200) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a random dropout layer\n",
    "tweet_encoder = Dropout(0.5)(tweet_encoder)\n",
    "tweet_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv1d/Relu:0' shape=(None, 67, 128) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add convolutin\n",
    "bigram_branch = Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_max_pooling1d/Max:0' shape=(None, 128) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add max pooling\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "bigram_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_2/cond/Identity:0' shape=(None, 128) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add random dropout to this brunch\n",
    "bigram_branch = Dropout(0.5)(bigram_branch)\n",
    "bigram_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_3/cond/Identity:0' shape=(None, 256) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reapead these steps to add another brunch of convolution\n",
    "trigram_branch = Conv1D(filters=256, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "trigram_branch = Dropout(0.2)(trigram_branch)\n",
    "trigram_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_4/cond/Identity:0' shape=(None, 512) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add another brunch of convolution\n",
    "fourgram_branch = Conv1D(filters=512, kernel_size=5, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "fourgram_branch = Dropout(0.2)(fourgram_branch)\n",
    "fourgram_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate/concat:0' shape=(None, 896) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now concatenet outputs from convolution brunches (128+256+512=896)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add hidden dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_5/cond/Identity:0' shape=(None, 256) dtype=float32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add output dense layerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation/Sigmoid:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now use 'Model' to group layers into an object with training and inference features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x286e42d3fc8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[tweet_input], outputs=[output]) # creat model object\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # comple model object\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 69)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 69, 200)      20000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 69, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 67, 128)      76928       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 66, 256)      205056      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 65, 512)      512512      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 512)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 896)          0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          229632      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,024,385\n",
      "Trainable params: 21,024,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# let us see the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000286E4F4ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000286E4F4ACA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "114/114 [==============================] - 43s 375ms/step - loss: 0.6529 - accuracy: 0.7301\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 41s 356ms/step - loss: 0.5022 - accuracy: 0.7735\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 40s 350ms/step - loss: 0.4567 - accuracy: 0.7978\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 40s 353ms/step - loss: 0.4365 - accuracy: 0.8083\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 42s 368ms/step - loss: 0.4019 - accuracy: 0.8315\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 40s 350ms/step - loss: 0.3586 - accuracy: 0.8461\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 40s 351ms/step - loss: 0.3171 - accuracy: 0.8680\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 41s 364ms/step - loss: 0.2876 - accuracy: 0.8782\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 40s 350ms/step - loss: 0.2636 - accuracy: 0.8856\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 40s 352ms/step - loss: 0.2456 - accuracy: 0.9019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x286e41e21c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now fit the model to the training dataset\n",
    "model.fit(xtrain, y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000286E43481F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x00000286E43481F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "29/29 [==============================] - 0s 14ms/step\n",
      "EvaluateBinaryClassification Object Created\n",
      "\n",
      "Total Samples\t906\n",
      "Positive Samples\t222\n",
      "Negative Samples\t684\n",
      "True Positive\t143\n",
      "True Negative\t634\n",
      "False Positive\t50\n",
      "False Negative\t79\n",
      "Accuracy\t0.8576158940397351\n",
      "Precision\t0.7409326424870466\n",
      "Recall\t0.6441441441441441\n",
      "F1 Measure\t0.689156626506024\n",
      "Cohen Kappa Score\t0.5973999131926504\n",
      "Area Under Curve\t0.7855223644697329\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       684\n",
      "           1       0.74      0.64      0.69       222\n",
      "\n",
      "    accuracy                           0.86       906\n",
      "   macro avg       0.82      0.79      0.80       906\n",
      "weighted avg       0.85      0.86      0.85       906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(xtest,verbose=1)\n",
    "predicted = [int(round(x[0])) for x in p]\n",
    "actual = y_test\n",
    "\n",
    "ebc = EvaluateBinaryClassification(gnd_truths = actual, predictions = predicted)\n",
    "print(ebc.get_full_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebc.save_full_report(model_name='CNN_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mp = ModelPersistance(store_path = BASE + 'stored_models\\\\cnn_w2v_mincount10')\n",
    "mp.store_model(tokenizer=tokenizer, model=cnn_model, max_len=maxlen)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stored Model to Predict on Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mp = ModelPersistance(store_path = BASE + 'stored_models\\\\cnn_w2v_mincount10')\n",
    "tokenizer, cnn_model, maxlen = mp.restore_model()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Unknown Data and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "UNKNOWN_CSV = BASE+'prepro_hasoc_2020_en_test.csv'\n",
    "df_unk = pd.read_csv(UNKNOWN_CSV, encoding='utf8')\n",
    "df_unk.head(5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_unk = list(df_unk['text'].astype(str))\n",
    "xunk = tokenizer.texts_to_sequences(X_unk)\n",
    "xunk = pad_sequences(xunk, maxlen=maxlen)\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "p_unk = cnn_model.predict(xunk,verbose=0)\n",
    "p_unk[:10]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pred_unk = [int(round(x[0])) for x in p_unk]\n",
    "pred_unk = np.array(pred_unk)\n",
    "sum(pred_unk)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LANGUAGE = 'EN'\n",
    "SUBTASK_NAME = 'A'\n",
    "pred_fname = 'submission_{}_{}.csv'.format(LANGUAGE, SUBTASK_NAME)\n",
    "BASE+'Predictions\\\\'+pred_fname\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_unk[['tweet_id', 'task1', 'ID']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "i2t = ['NOT', 'HOF']\n",
    "df_unk['task1'] = [i2t[i] for i in pred_unk]\n",
    "df_unk = df_unk[['tweet_id', 'task1', 'ID']]\n",
    "df_unk.to_csv(BASE+'Predictions\\\\'+pred_fname, encoding='utf8', index=None)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
